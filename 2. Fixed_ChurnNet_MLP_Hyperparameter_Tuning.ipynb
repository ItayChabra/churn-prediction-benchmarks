{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2BoEFIVyPEe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, roc_auc_score\n",
        ")\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "!{sys.executable} -m pip install -q keras-tuner\n",
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ftaVm2vvyth"
      },
      "outputs": [],
      "source": [
        "df_raw = pd.read_csv(\"https://raw.githubusercontent.com/mahidul5130/ChurnNet_Deep_Learning_Enhanced_Customer_Churn-Prediction_in_Telecommunication_Industry/refs/heads/main/Churn-data-UCI%20Dataset(5000).csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DYXdsCzQs2o"
      },
      "outputs": [],
      "source": [
        "Encoder = \"Label Encoder\"\n",
        "OverSamplingTecnique = \"SMOTE\"\n",
        "Model_Name = \"MLP-Attention\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYrRuOWWDlGt"
      },
      "source": [
        "**Label Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeJgfRf2zQ8N",
        "outputId": "d2564397-0449-4584-abfa-de4cbd09a777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying Label Encoder\n",
            "Label Encoder Transformation:\n",
            "internationalplan → [0 1]\n",
            "voicemailplan → [1 0]\n"
          ]
        }
      ],
      "source": [
        "if Encoder == \"Label Encoder\":\n",
        "    print(\"Applying Label Encoder\")\n",
        "    df_final = df_raw.copy()\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    text_data_features = ['internationalplan', 'voicemailplan']\n",
        "\n",
        "    print('Label Encoder Transformation:')\n",
        "    for i in text_data_features:\n",
        "        df_final[i] = le.fit_transform(df_final[i])\n",
        "        print(i, '→', df_final[i].unique())\n",
        "\n",
        "    X = df_final.drop(['churn'], axis=1).copy()\n",
        "    Y = df_final['churn'].copy().astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ane99tYEIOh"
      },
      "source": [
        "**One-hot Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv3ldE80D3ZE"
      },
      "outputs": [],
      "source": [
        "if Encoder == \"One-hot Encoder\":\n",
        "    print(\"Applying One-hot Encoder\")\n",
        "\n",
        "    categorical_columns = ['internationalplan', 'voicemailplan']\n",
        "\n",
        "    encoder = OneHotEncoder()\n",
        "    encoded_features = encoder.fit_transform(df_raw[categorical_columns]).toarray()\n",
        "\n",
        "    numerical_features = df_raw.drop(categorical_columns + ['churn'], axis=1)\n",
        "    X = np.hstack((encoded_features, numerical_features))\n",
        "\n",
        "    Y = df_raw['churn'].values.astype(int)\n",
        "    X = X.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgY86usCiVaw"
      },
      "source": [
        "**MLP + Feature Attention Block**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z__CRDfozaXN"
      },
      "outputs": [],
      "source": [
        "def feature_attention(inputs, reduction_ratio=4):\n",
        "    \"\"\"Lightweight feature attention mechanism\"\"\"\n",
        "    hidden_units = max(1, inputs.shape[-1] // reduction_ratio)\n",
        "    x = layers.Dense(hidden_units, activation='relu')(inputs)\n",
        "    x = layers.Dense(inputs.shape[-1], activation='sigmoid')(x)\n",
        "    return layers.Multiply()([inputs, x])\n",
        "\n",
        "\n",
        "def build_mlp_att(hp, input_dim):\n",
        "    \"\"\"Build MLP with attention and tunable hyperparameters\"\"\"\n",
        "\n",
        "    # Tunable Hyperparameters (narrowed ranges for faster search)\n",
        "    hidden_units = hp.Choice('hidden_units', values=[128, 256])\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.4, step=0.1)\n",
        "    lr = hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])\n",
        "\n",
        "    # Regularization (narrowed ranges)\n",
        "    l1_reg = hp.Float('l1_regularization', min_value=1e-5, max_value=1e-3, sampling='log')\n",
        "    l2_reg = hp.Float('l2_regularization', min_value=1e-5, max_value=1e-3, sampling='log')\n",
        "\n",
        "    inputs = layers.Input(shape=(input_dim,))\n",
        "\n",
        "    # Feature Attention\n",
        "    x = feature_attention(inputs, reduction_ratio=4)\n",
        "\n",
        "    # MLP layers with regularization\n",
        "    for units in [hidden_units, hidden_units // 2, hidden_units // 4]:\n",
        "        x = layers.Dense(\n",
        "            units,\n",
        "            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg)\n",
        "        )(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.ReLU()(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    outputs = layers.Dense(\n",
        "        1,\n",
        "        activation='sigmoid',\n",
        "        kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "iw-ZEtd6S1a_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_hyperparameters(X, Y, OverSamplingTecnique):\n",
        "    \"\"\"Perform hyperparameter tuning once on a validation set\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PHASE 1: HYPERPARAMETER TUNING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Create a single train/val split for tuning\n",
        "    X_tune, X_val, Y_tune, Y_val = train_test_split(\n",
        "        X, Y, test_size=0.2, random_state=42, stratify=Y\n",
        "    )\n",
        "\n",
        "    # Scale\n",
        "    scaler = StandardScaler()\n",
        "    X_tune = scaler.fit_transform(X_tune)\n",
        "    X_val = scaler.transform(X_val)\n",
        "\n",
        "    # Apply oversampling to training data only\n",
        "    sampler = None\n",
        "    if OverSamplingTecnique == \"SMOTE\":\n",
        "        sampler = SMOTE(random_state=42)\n",
        "    elif OverSamplingTecnique == \"SMOTE-Tomek\":\n",
        "        sampler = SMOTETomek(random_state=42)\n",
        "    elif OverSamplingTecnique == \"SMOTE-Enn\":\n",
        "        sampler = SMOTEENN(random_state=42)\n",
        "\n",
        "    if sampler is not None:\n",
        "        print(f\"Applying {OverSamplingTecnique} oversampling...\")\n",
        "        X_tune, Y_tune = sampler.fit_resample(X_tune, Y_tune)\n",
        "\n",
        "    # Set up tuner with reduced trials for speed\n",
        "    tuner = kt.RandomSearch(\n",
        "        hypermodel=lambda hp: build_mlp_att(hp, input_dim=X_tune.shape[1]),\n",
        "        objective='val_auc',\n",
        "        max_trials=8,  # Reduced from 10\n",
        "        executions_per_trial=1,\n",
        "        overwrite=True,\n",
        "        directory='keras_tuner_dir',\n",
        "        project_name='mlp_attention_global'\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        monitor='val_loss'\n",
        "    )\n",
        "\n",
        "    print(\"Starting hyperparameter search\")\n",
        "    tuner.search(\n",
        "        X_tune, Y_tune,\n",
        "        epochs=30,  # Reduced from 20\n",
        "        batch_size=64,  # Increased for speed\n",
        "        validation_data=(X_val, Y_val),\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Get best hyperparameters\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\"BEST HYPERPARAMETERS FOUND:\")\n",
        "    print(\"-\"*50)\n",
        "    for param, value in best_hps.values.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    return best_hps"
      ],
      "metadata": {
        "id": "3n2kjLFES4pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-Fold Cross-Validation With Best Parameters**"
      ],
      "metadata": {
        "id": "pILNAMksS-DA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9uxxru9zgU1"
      },
      "outputs": [],
      "source": [
        "def kfold_with_best_params(X, Y, best_hps, OverSamplingTecnique):\n",
        "    \"\"\"K-fold CV using pre-tuned hyperparameters\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PHASE 2: K-FOLD CROSS-VALIDATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    num_folds = 10\n",
        "    metrics_dict = {\n",
        "        \"accuracy\": [], \"precision\": [], \"recall\": [],\n",
        "        \"f1\": [], \"mcc\": [], \"auc\": []\n",
        "    }\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "    fold_number = 1\n",
        "\n",
        "    for train_index, test_index in skf.split(X, Y):\n",
        "        print(f\"\\nFold {fold_number}/{num_folds}\")\n",
        "\n",
        "        # Split\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "        # Scale\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "        # Oversample\n",
        "        sampler = None\n",
        "        if OverSamplingTecnique == \"SMOTE\":\n",
        "            sampler = SMOTE(random_state=42)\n",
        "        elif OverSamplingTecnique == \"SMOTE-Tomek\":\n",
        "            sampler = SMOTETomek(random_state=42)\n",
        "        elif OverSamplingTecnique == \"SMOTE-Enn\":\n",
        "            sampler = SMOTEENN(random_state=42)\n",
        "\n",
        "        if sampler is not None:\n",
        "            X_train, Y_train = sampler.fit_resample(X_train, Y_train)\n",
        "\n",
        "        # Build model with best hyperparameters\n",
        "        model = build_mlp_att(best_hps, input_dim=X_train.shape[1])\n",
        "\n",
        "        # Train with early stopping\n",
        "        early_stopping = EarlyStopping(\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            monitor=\"val_loss\"\n",
        "        )\n",
        "\n",
        "        model.fit(\n",
        "            X_train, Y_train,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            verbose=0,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping]\n",
        "        )\n",
        "\n",
        "        # Predict\n",
        "        Y_pred = model.predict(X_test, verbose=0)\n",
        "        Y_pred_binary = np.round(Y_pred).flatten()\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics_dict[\"accuracy\"].append(accuracy_score(Y_test, Y_pred_binary))\n",
        "        metrics_dict[\"precision\"].append(precision_score(Y_test, Y_pred_binary, zero_division=0))\n",
        "        metrics_dict[\"recall\"].append(recall_score(Y_test, Y_pred_binary, zero_division=0))\n",
        "        metrics_dict[\"f1\"].append(f1_score(Y_test, Y_pred_binary, zero_division=0))\n",
        "        metrics_dict[\"mcc\"].append(matthews_corrcoef(Y_test, Y_pred_binary))\n",
        "\n",
        "        try:\n",
        "            auc_val = roc_auc_score(Y_test, Y_pred)\n",
        "        except:\n",
        "            auc_val = 0.5\n",
        "        metrics_dict[\"auc\"].append(auc_val)\n",
        "\n",
        "        print(f\"  Acc: {metrics_dict['accuracy'][-1]:.4f} | \"\n",
        "              f\"F1: {metrics_dict['f1'][-1]:.4f} | \"\n",
        "              f\"AUC: {metrics_dict['auc'][-1]:.4f}\")\n",
        "\n",
        "        fold_number += 1\n",
        "\n",
        "    # Print final results\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL RESULTS (10-FOLD CV)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Average Test Accuracy: {np.mean(metrics_dict['accuracy']):.4f} ± {np.std(metrics_dict['accuracy']):.4f}\")\n",
        "    print(f\"Average Precision:     {np.mean(metrics_dict['precision']):.4f} ± {np.std(metrics_dict['precision']):.4f}\")\n",
        "    print(f\"Average Recall:        {np.mean(metrics_dict['recall']):.4f} ± {np.std(metrics_dict['recall']):.4f}\")\n",
        "    print(f\"Average F1 Score:      {np.mean(metrics_dict['f1']):.4f} ± {np.std(metrics_dict['f1']):.4f}\")\n",
        "    print(f\"Average MCC:           {np.mean(metrics_dict['mcc']):.4f} ± {np.std(metrics_dict['mcc']):.4f}\")\n",
        "    print(f\"Average AUC-ROC:       {np.mean(metrics_dict['auc']):.4f} ± {np.std(metrics_dict['auc']):.4f}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return metrics_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_arr = X.values if isinstance(X, pd.DataFrame) else np.asarray(X)\n",
        "Y_arr = Y.values if isinstance(Y, (pd.Series, np.ndarray)) else np.asarray(Y)\n",
        "\n",
        "# Phase 1: Tune hyperparameters once\n",
        "best_hps = tune_hyperparameters(X_arr, Y_arr, OverSamplingTecnique)\n",
        "\n",
        "# Phase 2: Run K-fold with best hyperparameters\n",
        "final_metrics = kfold_with_best_params(X_arr, Y_arr, best_hps, OverSamplingTecnique)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQn6SrY8EjYF",
        "outputId": "892ac34c-7255-4fbc-c7d9-6e06c83330cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 8 Complete [00h 00m 25s]\n",
            "val_auc: 0.9224357008934021\n",
            "\n",
            "Best val_auc So Far: 0.932306170463562\n",
            "Total elapsed time: 00h 04m 06s\n",
            "\n",
            "--------------------------------------------------\n",
            "BEST HYPERPARAMETERS FOUND:\n",
            "--------------------------------------------------\n",
            "  hidden_units: 256\n",
            "  dropout_rate: 0.4\n",
            "  learning_rate: 0.001\n",
            "  l1_regularization: 8.639199562623076e-05\n",
            "  l2_regularization: 1.7775537216490075e-05\n",
            "--------------------------------------------------\n",
            "\n",
            "==================================================\n",
            "PHASE 2: K-FOLD CROSS-VALIDATION\n",
            "==================================================\n",
            "\n",
            "Fold 1/10\n",
            "  Acc: 0.9460 | F1: 0.8029 | AUC: 0.9005\n",
            "\n",
            "Fold 2/10\n",
            "  Acc: 0.9540 | F1: 0.8414 | AUC: 0.9211\n",
            "\n",
            "Fold 3/10\n",
            "  Acc: 0.9660 | F1: 0.8794 | AUC: 0.9509\n",
            "\n",
            "Fold 4/10\n",
            "  Acc: 0.9340 | F1: 0.7898 | AUC: 0.9407\n",
            "\n",
            "Fold 5/10\n",
            "  Acc: 0.9260 | F1: 0.7730 | AUC: 0.9517\n",
            "\n",
            "Fold 6/10\n",
            "  Acc: 0.9260 | F1: 0.7613 | AUC: 0.9217\n",
            "\n",
            "Fold 7/10\n",
            "  Acc: 0.9560 | F1: 0.8472 | AUC: 0.9133\n",
            "\n",
            "Fold 8/10\n",
            "  Acc: 0.9240 | F1: 0.7246 | AUC: 0.8893\n",
            "\n",
            "Fold 9/10\n",
            "  Acc: 0.9480 | F1: 0.8194 | AUC: 0.9171\n",
            "\n",
            "Fold 10/10\n",
            "  Acc: 0.9500 | F1: 0.8148 | AUC: 0.8967\n",
            "\n",
            "==================================================\n",
            "FINAL RESULTS (10-FOLD CV)\n",
            "==================================================\n",
            "Average Test Accuracy: 0.9430 ± 0.0139\n",
            "Average Precision:     0.7865 ± 0.0640\n",
            "Average Recall:        0.8303 ± 0.0563\n",
            "Average F1 Score:      0.8054 ± 0.0431\n",
            "Average MCC:           0.7742 ± 0.0497\n",
            "Average AUC-ROC:       0.9203 ± 0.0208\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}